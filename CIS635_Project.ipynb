{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GVSU-CIS635/term-project-proposal-plato/blob/main/CIS635_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Thematic and Argument Structure Analysis of Platoâ€™s Republic Using Data Mining Techniques\n",
        "\n",
        "Tanishq Daniel, Trevor Ouma, Nate Miller"
      ],
      "metadata": {
        "id": "VrbyD296ZxDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Where is the .txt file, is it stored locally on your machine or you uploaded it here?**\n"
      ],
      "metadata": {
        "id": "Lz8vTR_s3CSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1tLJPa6blTx",
        "outputId": "2ca500fe-942a-4cc7-ef97-55514a37e114"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W4OJsuuffvA",
        "outputId": "30e241e3-13ee-4b4e-ce11-253063f3b167"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO remove introductory content, stop words and punctiation that are not relevant\n",
        "def preprocess_text(text_data):\n",
        "    documents = text_data.split('\\n\\n')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    punctuation = set(string.punctuation)\n",
        "\n",
        "    cleaned_texts = []\n",
        "    for document in documents:\n",
        "        tokens = word_tokenize(document.lower())\n",
        "        cleaned_tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
        "        cleaned_texts.append(' '.join(cleaned_tokens))\n",
        "\n",
        "    return cleaned_texts"
      ],
      "metadata": {
        "id": "rxBZeMCebpAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO remove introductory content, stop words and punctiation that are not relevant but data is split by \"books\"\n",
        "def preprocess_text_by_book(text_data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    document_books = text_data.split(\"BOOK\")\n",
        "    relevant_books = document_books[12:22]\n",
        "\n",
        "    books_data = []\n",
        "    for i, book_content in enumerate(relevant_books[:10], 1):\n",
        "        book_content = re.sub(r'\\b[MCDXLVI]+\\b', '', book_content)\n",
        "        book_content = re.sub(r'[^\\w\\s]', '', book_content).lower()\n",
        "        book_data = {\n",
        "            \"book_number\": i,\n",
        "            \"content\": book_content.strip()\n",
        "        }\n",
        "        books_data.append(book_data)\n",
        "\n",
        "    return books_data"
      ],
      "metadata": {
        "id": "6xcVA5s7jDGX"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_data = preprocess_text_by_book(text_data)\n",
        "\n",
        "for book in books_data:\n",
        "    print(f\"BOOK {book['book_number']} Content Preview:\\n{book['content'][:200]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS0WiowSo0Xu",
        "outputId": "b78b3c5e-f824-4e55-92c2-0065effa5467"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK 1 Content Preview:\n",
            "went down yesterday to the piraeus with glaucon the son of ariston\n",
            "that  might offer up my prayers to the goddess bendis the thracian\n",
            "artemis and also because  wanted to see in what manner they would\n",
            "...\n",
            "\n",
            "BOOK 2 Content Preview:\n",
            "with these words  was thinking that  had made an end of the\n",
            "discussion but the end in truth proved to be only a beginning for\n",
            "glaucon who is always the most pugnacious of men was dissatisfied at\n",
            "thras...\n",
            "\n",
            "BOOK 3 Content Preview:\n",
            "such then  said are our principles of theologysome tales are to be\n",
            "told and others are not to be told to our disciples from their youth\n",
            "upwards if we mean them to honour the gods and their parents and...\n",
            "\n",
            "BOOK 4 Content Preview:\n",
            "here adeimantus interposed a question how would you answer socrates\n",
            "said he if a person were to say that you are making these people\n",
            "miserable and that they are the cause of their own unhappiness the\n",
            "...\n",
            "\n",
            "BOOK 5 Content Preview:\n",
            "such is the good and true city or state and the good and true man is\n",
            "of the same pattern and if this is right every other is wrong and the\n",
            "evil is one which affects not only the ordering of the state ...\n",
            "\n",
            "BOOK 6 Content Preview:\n",
            "and thus glaucon after the argument has gone a weary way the true and\n",
            "the false philosophers have at length appeared in view\n",
            "\n",
            " do not think he said that the way could have been shortened\n",
            "\n",
            " suppose not...\n",
            "\n",
            "BOOK 7 Content Preview:\n",
            "and now  said let me show in a figure how far our nature is\n",
            "enlightened or unenlightenedbehold human beings living in a\n",
            "underground den which has a mouth open towards the light and reaching\n",
            "all along ...\n",
            "\n",
            "BOOK 8 Content Preview:\n",
            "and so glaucon we have arrived at the conclusion that in the perfect\n",
            "state wives and children are to be in common and that all education\n",
            "and the pursuits of war and peace are also to be common and the...\n",
            "\n",
            "BOOK 9 Content Preview:\n",
            "last of all comes the tyrannical man about whom we have once more to\n",
            "ask how is he formed out of the democratical and how does he live in\n",
            "happiness or in misery\n",
            "\n",
            "yes he said he is the only one remaini...\n",
            "\n",
            "BOOK 10 Content Preview:\n",
            "of the many excellences which  perceive in the order of our state\n",
            "there is none which upon reflection pleases me better than the rule\n",
            "about poetry\n",
            "\n",
            "to what do you refer\n",
            "\n",
            "to the rejection of imitative ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the words\n",
        "def token_text_by_book(text_data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    document_books = text_data.split(\"BOOK\")\n",
        "    relevant_books = document_books[12:22]\n",
        "\n",
        "    books_data_token = []\n",
        "    for i, book_content in enumerate(relevant_books, 1):\n",
        "        book_content = re.sub(r'\\b[MCDXLVI]+\\b', '', book_content)\n",
        "        book_content = re.sub(r'[^\\w\\s]', '', book_content).lower()\n",
        "        tokens = word_tokenize(book_content)\n",
        "        clean_tokens = [word for word in tokens if word not in stop_words]\n",
        "        book_data_token = {\n",
        "            \"book_number\": i,\n",
        "            \"clean_tokens\": clean_tokens\n",
        "        }\n",
        "        books_data_token.append(book_data_token)\n",
        "\n",
        "    return books_data_token"
      ],
      "metadata": {
        "id": "xSk-qzthoL0a"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_data = token_text_by_book(text_data)\n",
        "\n",
        "for book in token_data:\n",
        "    print(f\"BOOK {book['book_number']} Clean Tokens Preview:\\n{book['clean_tokens'][:20]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jezJY7fBbmJ0",
        "outputId": "b84251cf-bbf1-477a-da48-f09c5649846c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK 1 Clean Tokens Preview:\n",
            "['went', 'yesterday', 'piraeus', 'glaucon', 'son', 'ariston', 'might', 'offer', 'prayers', 'goddess', 'bendis', 'thracian', 'artemis', 'also', 'wanted', 'see', 'manner', 'would', 'celebrate', 'festival']...\n",
            "\n",
            "BOOK 2 Clean Tokens Preview:\n",
            "['words', 'thinking', 'made', 'end', 'discussion', 'end', 'truth', 'proved', 'beginning', 'glaucon', 'always', 'pugnacious', 'men', 'dissatisfied', 'thrasymachus', 'retirement', 'wanted', 'battle', 'said', 'socrates']...\n",
            "\n",
            "BOOK 3 Clean Tokens Preview:\n",
            "['said', 'principles', 'theologysome', 'tales', 'told', 'others', 'told', 'disciples', 'youth', 'upwards', 'mean', 'honour', 'gods', 'parents', 'value', 'friendship', 'one', 'another', 'yes', 'think']...\n",
            "\n",
            "BOOK 4 Clean Tokens Preview:\n",
            "['adeimantus', 'interposed', 'question', 'would', 'answer', 'socrates', 'said', 'person', 'say', 'making', 'people', 'miserable', 'cause', 'unhappiness', 'city', 'fact', 'belongs', 'none', 'better', 'whereas']...\n",
            "\n",
            "BOOK 5 Clean Tokens Preview:\n",
            "['good', 'true', 'city', 'state', 'good', 'true', 'man', 'pattern', 'right', 'every', 'wrong', 'evil', 'one', 'affects', 'ordering', 'state', 'also', 'regulation', 'individual', 'soul']...\n",
            "\n",
            "BOOK 6 Clean Tokens Preview:\n",
            "['thus', 'glaucon', 'argument', 'gone', 'weary', 'way', 'true', 'false', 'philosophers', 'length', 'appeared', 'view', 'think', 'said', 'way', 'could', 'shortened', 'suppose', 'said', 'yet']...\n",
            "\n",
            "BOOK 7 Clean Tokens Preview:\n",
            "['said', 'let', 'show', 'figure', 'far', 'nature', 'enlightened', 'unenlightenedbehold', 'human', 'beings', 'living', 'underground', 'den', 'mouth', 'open', 'towards', 'light', 'reaching', 'along', 'den']...\n",
            "\n",
            "BOOK 8 Clean Tokens Preview:\n",
            "['glaucon', 'arrived', 'conclusion', 'perfect', 'state', 'wives', 'children', 'common', 'education', 'pursuits', 'war', 'peace', 'also', 'common', 'best', 'philosophers', 'bravest', 'warriors', 'kings', 'replied']...\n",
            "\n",
            "BOOK 9 Clean Tokens Preview:\n",
            "['last', 'comes', 'tyrannical', 'man', 'ask', 'formed', 'democratical', 'live', 'happiness', 'misery', 'yes', 'said', 'one', 'remaining', 'however', 'said', 'previous', 'question', 'remains', 'unanswered']...\n",
            "\n",
            "BOOK 10 Clean Tokens Preview:\n",
            "['many', 'excellences', 'perceive', 'order', 'state', 'none', 'upon', 'reflection', 'pleases', 'better', 'rule', 'poetry', 'refer', 'rejection', 'imitative', 'poetry', 'certainly', 'ought', 'received', 'see']...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO tune hyperparameters, separate topics into themes and subthemes, return topics and paragraphs with high distribution\n",
        "def extract_themes(text):\n",
        "  # Create Document-Term Matrix with adjusted min_df and max_df\n",
        "  vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "  term_matrix = vectorizer.fit_transform(text)\n",
        "\n",
        "  # Fit LDA\n",
        "  lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "  lda.fit(term_matrix)\n",
        "\n",
        "  # Display Topics\n",
        "  terms = vectorizer.get_feature_names_out()\n",
        "  def display_topics(model, feature_names, no_top_words):\n",
        "      for topic_idx, topic in enumerate(model.components_):\n",
        "          print(f\"Topic {topic_idx}:\")\n",
        "          print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "  display_topics(lda, terms, 5)\n",
        "\n",
        "  # Document-Topic Distribution\n",
        "  distributions = lda.transform(term_matrix)\n",
        "  for idx, distribution in enumerate(distributions):\n",
        "      print(f\"Document {idx} topic distribution: {distribution}\")"
      ],
      "metadata": {
        "id": "CiqoyxvX2RhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "conclusion_keywords = {\"therefore\", \"thus\", \"hence\", \"consequently\", \"in conclusion\"}\n",
        "premise_keywords = {\"because\", \"since\", \"for\", \"as\", \"insofar as\", \"inasmuch as\"}\n",
        "\n",
        "def extract_arguments(documents):\n",
        "    arguments = []\n",
        "\n",
        "    for doc in documents:\n",
        "        processed_doc = nlp(doc)\n",
        "\n",
        "        premises = []\n",
        "        conclusions = []\n",
        "\n",
        "        for sent in processed_doc.sents:\n",
        "            sentence_text = sent.text.lower()\n",
        "\n",
        "            if any(keyword in sentence_text for keyword in conclusion_keywords):\n",
        "                conclusions.append(sentence_text)\n",
        "\n",
        "            elif any(keyword in sentence_text for keyword in premise_keywords):\n",
        "                premises.append(sentence_text)\n",
        "\n",
        "        if premises or conclusions:\n",
        "            arguments.append({\n",
        "                \"text\": doc,\n",
        "                \"premises\": premises,\n",
        "                \"conclusions\": conclusions\n",
        "            })\n",
        "\n",
        "    argument_df = pd.DataFrame(arguments)\n",
        "    print(argument_df.head(10))\n"
      ],
      "metadata": {
        "id": "-_Sms6kwb12t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_theme_complexity():\n",
        "  print('calculate theme complexity')"
      ],
      "metadata": {
        "id": "QGf4zaw0gUGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_argument_complexity():\n",
        "  print('calculate argument complexity')"
      ],
      "metadata": {
        "id": "6IgbNNNvgUqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_complexity():\n",
        "  print('calculate complexity')"
      ],
      "metadata": {
        "id": "Us5FWmhlgU65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlation():\n",
        "  print('calculate correlation')"
      ],
      "metadata": {
        "id": "fntqy5L6gVNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualization():\n",
        "  print('visualization')"
      ],
      "metadata": {
        "id": "ZT_1zDmvgVjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5bqIOVCihhPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(text_data):\n",
        "    # Step 1: Preprocess the text\n",
        "    documents = preprocess_text(text_data)\n",
        "\n",
        "    # Step 2: Detect Themes\n",
        "    extract_themes(documents)\n",
        "\n",
        "    # Step 3: Detect Arguments\n",
        "    extract_arguments(documents)\n",
        "\n",
        "    # Step 4: Calculate Theme Complexity\n",
        "    calculate_theme_complexity()\n",
        "\n",
        "    # Step 5: Calculate Argument Complexity\n",
        "    calculate_argument_complexity()\n",
        "\n",
        "    # Step 6: Correlation Analysis\n",
        "    calculate_correlation()\n",
        "\n",
        "    # Step 7: Visualization\n",
        "    visualization()"
      ],
      "metadata": {
        "id": "WxhXuDMFzOcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "data = urlopen('https://raw.githubusercontent.com/GVSU-CIS635/Datasets/refs/heads/master/republic.txt')\n",
        "html_response = data.read()\n",
        "encoding = data.headers.get_content_charset('utf-8')\n",
        "text_data = html_response.decode(encoding)\n",
        "\n",
        "#main(text_data)\n"
      ],
      "metadata": {
        "id": "sgiInTUyhiTA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMbsZ3mMT4QU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}